{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "In this exercise, we will explore a movie review dataset.\n",
    "\n",
    "\n",
    "**Task 1:** Load the data from `/dsa/data/all_datasets/movie_reviews` into mvr variable. While loading use `encoding='utf-8'`. (Solved for you)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/dsa/data/all_datasets/movie_reviews'\n",
    "\n",
    "mvr = load_files(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 2000\n"
     ]
    }
   ],
   "source": [
    "print('Number of Reviews: {0}'.format(len(mvr.filenames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Apply `SentimentIntensityAnalyzer` on the entire dataset to estimate polarity scores. Print the top 3 `positive`, `negative`, and `neural` reviews based on the following rule: \n",
    "\n",
    "\n",
    "* positive sentiment: compound score >= 0.05\n",
    "* neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "* negative sentiment: compound score <= -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in mvr.data:\n",
    "    review_text = review.decode('utf-8')\n",
    "    sentiment = sia.polarity_scores(review_text)\n",
    "    sentiments.append((sentiment['compound'],review_text))\n",
    "\n",
    "sentiments = sorted(sentiments, key=lambda x: x[0], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Positive Reviews:\n",
      "Score: 0.9999, Review:as i write the review for the new hanks/ryan romantic comedy you've got mail , i am acutely aware that i am typing it on a computer and sending it a billion miles away on the internet . \n",
      "i am also awa...\n",
      "Score: 0.9999, Review:note : some may consider portions of the following text to be spoilers . \n",
      "be forewarned . \n",
      "the teaser trailers for my best friend's wedding scarsely gave reason for hope - it looked like the sort of g...\n",
      "Score: 0.9998, Review:most people fit into two different categories : you either love woody allen , or you hate his guts . \n",
      "my family , for the most part , hates him and his movies . \n",
      "i think he's very funny , but his shti...\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 3 Positive Reviews:\")\n",
    "for sentiment in sentiments[:3]:\n",
    "    print (f\"Score: {sentiment[0]}, Review:{sentiment[1][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Negative Reviews:\n",
      "socre: -0.9996, Review: natural born killers is really a very simple story that , in essence , has already been told in bonnie & clyde with some major variations in emphasis , mood and degree . \n",
      "both films glamorize \" outlaw...\n",
      "socre: -0.9996, Review: weighed down by tired plot lines and spielberg's reliance on formulas , _saving private ryan_ is a mediocre film which nods in the direction of realism before descending into an abyss of cliches . \n",
      "th...\n",
      "socre: -0.9997, Review: the above is dialogue from this film , taken almost completely in context , and not jazzed up a bit to make it more inept than it is . \n",
      "it is spoken between two of the protagonists somewhere in the fi...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 3 Negative Reviews:\")\n",
    "for sentiment in sentiments[-3:]:\n",
    "    print(f\"socre: {sentiment[0]}, Review: {sentiment[1][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Neutral Reviews:\n",
      "Score: -0.0488, Review: pulp fiction , quentin tarantino's anxiously awaited and superb follow-up to reservoir dogs , is absolutely and without a doubt progressing as one of the most talked about , loved , and hated films of...\n"
     ]
    }
   ],
   "source": [
    "neutral_reviews = [s for s in sentiments if -0.05 < s[0] < 0.05]\n",
    "print(\"\\nTop 3 Neutral Reviews:\")\n",
    "for sentiment in neutral_reviews[:3]:\n",
    "    print(f\"Score: {sentiment[0]}, Review: {sentiment[1][:200]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Apply `SentimentIntensityAnalyzer` on the entire dataset to estimate polarity scores. Print a classification report based on the following rule: \n",
    "\n",
    "\n",
    "positive sentiment: compound score >= 0\n",
    "negative sentiment: compound score < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.31      0.47      2000\n",
      "    Positive       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.31      2000\n",
      "   macro avg       0.50      0.15      0.24      2000\n",
      "weighted avg       1.00      0.31      0.47      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for review in mvr.data:\n",
    "    review_text = review.decode('utf-8')\n",
    "    sentiment_score = sia.polarity_scores(review_text)['compound']\n",
    "    \n",
    "    true_label = 1 if review_text in mvr.target_names[1] else 0\n",
    "    true_labels.append(true_label)\n",
    "    \n",
    "    if sentiment_score >=0:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, target_names=['Negative', 'Positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
