{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will go over `realDonaldTrump_tweets` and perform topic modeling. Each line in this file is a tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Load the data**\n",
    "\n",
    "Consider each tweet as a document. Load the tweets. Strip away symbols and web links in the tweets. If the tweet becomes empty string after preprocessing, then discard the tweet from analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/dsa/data/all_datasets/linguistic/realDonaldTrump_tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each tweet as a document\n",
    "with open(file_path, 'r') as f:\n",
    "    tweets = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'https?:\\/\\/[^\\s]+', '', tweet)\n",
    "    tweet = re.sub(r\"[^a-zA-Z\\s]\",\"\",tweet)\n",
    "    tweet = tweet.strip().lower()\n",
    "    return tweet\n",
    "\n",
    "cleaned_tweets = [preprocess_tweet(tweet) for tweet in tweets if preprocess_tweet(tweet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 preprocessed tweets: ['it was a great honor to have spoken before the countries of the world at the united nations', 'usaatungaunga', 'god bless the people of mexico city we are with you and will be there for you', 'as president of the united states of america i will always put americafirstunga', 'full remarks']\n",
      "The amount of preprocessed tweets:3651\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 preprocessed tweets:\", cleaned_tweets[:5])\n",
    "\n",
    "print(f\"The amount of preprocessed tweets:{len(cleaned_tweets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Create term frequency matrix for these tweets.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF demensions of matrix: (3651, 1000)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_tweets)  \n",
    "\n",
    "\n",
    "print(f\"TF-IDF demensions of matrix: {tfidf_matrix.shape}\")\n",
    "\n",
    "\n",
    "print(tfidf_matrix[:5].toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Apply LDA topic modeling method with 5 topics**\n",
    "\n",
    "Fit an LDA model with 5 topics on these tweets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "trump great amp thank people american united day states president\n",
      "Topic 1:\n",
      "rt obamacare amp vote realdonaldtrump debates teamtrump healthcare hillaryclinton trumppence\n",
      "Topic 2:\n",
      "great america foxandfriends make enjoy today president rt honor jobs\n",
      "Topic 3:\n",
      "makeamericagreatagain clinton hillary watch imwithyou crooked media cnn news fake\n",
      "Topic 4:\n",
      "thank join maga pm tickets draintheswamp florida tomorrow ohio americafirst\n"
     ]
    }
   ],
   "source": [
    "  # Add your code below\n",
    "# -------------------\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "display_topics(lda, tfidf_feature_names, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Print the top 10 words for each of the topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "pm clinton hillary media news watch enjoy foxandfriends fake rt\n",
      "Topic 2:\n",
      "thank great join maga draintheswamp america tickets make florida tomorrow\n",
      "Topic 3:\n",
      "trump americafirst obamacare debates crookedhillary amp healthcare donald hillaryclinton repeal\n",
      "Topic 4:\n",
      "makeamericagreatagain hillary states jobs amp national clinton years obama happy\n",
      "Topic 5:\n",
      "rt amp north realdonaldtrump people president great failing korea trump\n"
     ]
    }
   ],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "no_top_words = 10\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "display_topics(lda, tfidf_feature_names, no_top_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5: Name each of the topic (No right answer)**\n",
    "\n",
    "After observing top-10 words in each topic, do these topics make sense to you? Can you name each of the topic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: Politics\n",
      "Topic 2: Economy\n",
      "Topic 3: Healthcare\n",
      "Topic 4: Foreign Policy\n",
      "Topic 5: Social Media\n"
     ]
    }
   ],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "\n",
    "topic_names = {\n",
    "    1: \"Politics\",\n",
    "    2: \"Economy\",\n",
    "    3: \"Healthcare\",\n",
    "    4: \"Foreign Policy\",\n",
    "    5: \"Social Media\"\n",
    "}\n",
    "\n",
    "for topic_idx, topic_name in topic_names.items():\n",
    "    print(f\"Topic {topic_idx}: {topic_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6: Create a TFIDF matrix**\n",
    "\n",
    "Create TFIDF matrix for these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Matrix Shape: (3998, 3083)\n"
     ]
    }
   ],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tweets)\n",
    "\n",
    "print('TFIDF Matrix Shape: {}'.format(tfidf_matrix.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6: Apply NMF topic modeling with 5 topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "https tickets watch draintheswamp 3kwol2ibaw americafirst crookedhillary icymi rt video\n",
      "Topic 1:\n",
      "great america make safe going honor day people today state\n",
      "Topic 2:\n",
      "makeamericagreatagain imwithyou erictrump lets movement lesm pennsylvania join poll nfib\n",
      "Topic 3:\n",
      "thank maga americafirst florida ohio join imwithyou trumppence16 support new\n",
      "Topic 4:\n",
      "amp rt hillary trump clinton realdonaldtrump people draintheswamp president media\n"
     ]
    }
   ],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=5, random_state=42)\n",
    "\n",
    "nmf_model.fit(tfidf_matrix)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "display_topics(nmf_model, tfidf_feature_names, 10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7: Print the top 10 words for each of the topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "https tickets watch draintheswamp 3kwol2ibaw americafirst crookedhillary icymi rt video\n",
      "Topic 1:\n",
      "great america make safe going honor day people today state\n",
      "Topic 2:\n",
      "makeamericagreatagain imwithyou erictrump lets movement lesm pennsylvania join poll nfib\n",
      "Topic 3:\n",
      "thank maga americafirst florida ohio join imwithyou trumppence16 support new\n",
      "Topic 4:\n",
      "amp rt hillary trump clinton realdonaldtrump people draintheswamp president media\n"
     ]
    }
   ],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "display_topics(nmf_model, tfidf_feature_names, 10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8: Perform a comparison between the topics identified by LDA and NMF methods.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your code below\n",
    "# -------------------\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "nmf_model = NMF(n_components=5, random_state=42)\n",
    "nmf_model.fit(tfidf_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n",
      "Topic 0:\n",
      "thank great https america join make maga rt realdonaldtrump amp\n",
      "Topic 1:\n",
      "https makeamericagreatagain draintheswamp bigleaguetruth clinton hillary debates2016 vpdebate rt bernie\n",
      "Topic 2:\n",
      "tickets media hillary news fake clinton amp people years healthcare\n",
      "Topic 3:\n",
      "https americafirst watch imwithyou thank crookedhillary amp great president united\n",
      "Topic 4:\n",
      "https enjoy amp today interviewed 00 great whitehouse foxandfriends rt\n",
      "\n",
      "NMF Topics:\n",
      "Topic 0:\n",
      "https tickets watch draintheswamp 3kwol2ibaw americafirst crookedhillary icymi rt video\n",
      "Topic 1:\n",
      "great america make safe going honor day people today state\n",
      "Topic 2:\n",
      "makeamericagreatagain imwithyou erictrump lets movement lesm pennsylvania join poll nfib\n",
      "Topic 3:\n",
      "thank maga americafirst florida ohio join imwithyou trumppence16 support new\n",
      "Topic 4:\n",
      "amp rt hillary trump clinton realdonaldtrump people draintheswamp president media\n"
     ]
    }
   ],
   "source": [
    "def display_topics_comparison(lda_model, nmf_model, feature_names, no_top_words):\n",
    "    print(\"LDA Topics:\")\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    \n",
    "    print(\"\\nNMF Topics:\")\n",
    "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "display_topics_comparison(lda_model, nmf_model, tfidf_feature_names, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.9886px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
